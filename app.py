import streamlit as st
import chromadb
from chromadb.utils import embedding_functions
from sentence_transformers import SentenceTransformer
import pysrt
import re
import logging
from typing import List, Dict, Any
from dataclasses import dataclass
from pathlib import Path
import time
import shutil

# --- CONFIGURATION ---
SCRIPT_DIR = Path(__file__).parent.absolute()

@dataclass
class AppConfig:
    DB_PATH: str = str(SCRIPT_DIR / "chroma_db")
    COLLECTION_NAME: str = "anime_transcripts"
    
    MODEL_NAME: str = "paraphrase-multilingual-MiniLM-L12-v2"
    # -----------------------
    
    SUBS_FOLDER_NAME: str = "aot_subs"
    BATCH_SIZE: int = 2000 

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# --- CORE LOGIC ---

class SubtitleProcessor:
    @staticmethod
    def clean_text(text: str) -> str:
        text = re.sub(r'[^\w\s\.\!\?–∞-—è–ê-–Ø—ë–Å]', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text.lower()

    @staticmethod
    def _process_subs_object(subs, file_identifier: str) -> List[Dict[str, Any]]:
        transcripts = []
        for i, sub in enumerate(subs):
            if len(sub.text.strip()) < 3:
                continue
            transcripts.append({
                'episode': file_identifier,
                'text': SubtitleProcessor.clean_text(sub.text),
                'raw_text': sub.text,
                'start': str(sub.start),
                'end': str(sub.end),
                'duration': sub.duration.seconds,
                'index': i,
                'id': f"{file_identifier}_{i}"
            })
        return transcripts

    @staticmethod
    def parse_file_path(file_path: Path) -> List[Dict[str, Any]]:
        try:
            subs = pysrt.open(str(file_path), encoding='utf-8')
            return SubtitleProcessor._process_subs_object(subs, file_path.stem)
        except Exception as e:
            try:
                subs = pysrt.open(str(file_path), encoding='cp1251')
                return SubtitleProcessor._process_subs_object(subs, file_path.stem)
            except:
                logger.error(f"Error parsing file {file_path}: {e}")
                return []

    @staticmethod
    def parse_uploaded_file(uploaded_file) -> List[Dict[str, Any]]:
        try:
            content = uploaded_file.getvalue().decode("utf-8", errors="ignore")
            subs = pysrt.from_string(content)
            file_id = Path(uploaded_file.name).stem
            return SubtitleProcessor._process_subs_object(subs, file_id)
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {uploaded_file.name}: {e}")
            return []

class VectorSearchService:
    def __init__(self, config: AppConfig):
        self.config = config
        self.client = chromadb.PersistentClient(path=config.DB_PATH)
        self.embedding_fn = embedding_functions.SentenceTransformerEmbeddingFunction(
            model_name=config.MODEL_NAME
        )
        self.collection = self.client.get_or_create_collection(
            name=config.COLLECTION_NAME,
            metadata={"hnsw:space": "cosine"},
            embedding_function=self.embedding_fn
        )

    def reset_collection(self):
        try:
            self.client.delete_collection(self.config.COLLECTION_NAME)
            self.collection = self.client.get_or_create_collection(
                name=self.config.COLLECTION_NAME,
                metadata={"hnsw:space": "cosine"},
                embedding_function=self.embedding_fn
            )
        except Exception as e:
            logger.error(f"Reset error: {e}")

    def index_data(self, data: List[Dict[str, Any]]) -> int:
        if not data:
            return 0
            
        ids = [item['id'] for item in data]
        documents = [item['text'] for item in data]
        metadatas = [{
            'episode': item['episode'],
            'start': item['start'],
            'end': item['end'],
            'raw_text': item['raw_text']
        } for item in data]

        total = len(ids)
        batch_size = self.config.BATCH_SIZE
        
        for i in range(0, total, batch_size):
            end = min(i + batch_size, total)
            self.collection.upsert(
                ids=ids[i:end],
                documents=documents[i:end],
                metadatas=metadatas[i:end]
            )
        return total

    def search(self, query: str, n_results: int = 5) -> List[Dict[str, Any]]:
        clean_query = SubtitleProcessor.clean_text(query)
        try:
            results = self.collection.query(
                query_texts=[clean_query],
                n_results=n_results,
                include=['documents', 'metadatas', 'distances']
            )
            formatted = []
            if results['ids'] and results['ids'][0]:
                for i in range(len(results['ids'][0])):
                    formatted.append({
                        'id': results['ids'][0][i],
                        'text': results['documents'][0][i],
                        'metadata': results['metadatas'][0][i],
                        'score': (1 - results['distances'][0][i]) * 100
                    })
            return formatted
        except Exception as e:
            logger.error(f"Search error: {e}")
            return []

    def get_stats(self) -> int:
        return self.collection.count()

# --- UI ---

def render_result(result: Dict[str, Any]):
    meta = result['metadata']
    score = result['score']
    with st.container(border=True):
        c1, c2 = st.columns([0.85, 0.15])
        with c1:
            st.markdown(f"**üé¨ {meta['episode']}** `({meta['start']} -> {meta['end']})`")
            st.markdown(f"### \"{meta['raw_text']}\"")
        with c2:
            st.metric("Score", f"{score:.0f}%")

def main():
    st.set_page_config(page_title="Subtitle Search", page_icon="üîé", layout="wide")
    
    if 'config' not in st.session_state:
        st.session_state.config = AppConfig()
    
    @st.cache_resource
    def get_service():
        return VectorSearchService(st.session_state.config)
    
    service = get_service()

    with st.sidebar:
        st.header("‚öôÔ∏è –ò—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö")
        
        mode = st.radio("–û—Ç–∫—É–¥–∞ –±—Ä–∞—Ç—å —Å—É–±—Ç–∏—Ç—Ä—ã?", ["üìÅ –î–µ–º–æ (AoT)", "üì§ –ó–∞–≥—Ä—É–∑–∏—Ç—å —Å–≤–æ–∏ (.srt)"])
        
        if mode == "üìÅ –î–µ–º–æ (AoT)":
            full_path = SCRIPT_DIR / st.session_state.config.SUBS_FOLDER_NAME
            st.info(f"–ü–∞–ø–∫–∞ –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ: `{st.session_state.config.SUBS_FOLDER_NAME}`")
            
            if st.button("üîÑ –ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å –î–µ–º–æ", type="primary"):
                with st.status("–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è...", expanded=True) as status:
                    if not full_path.exists():
                        st.error("–ü–∞–ø–∫–∞ –¥–µ–º–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")
                        st.stop()
                    
                    files = list(full_path.glob("*.srt"))
                    all_transcripts = []
                    service.reset_collection() 
                    
                    prog = st.progress(0)
                    for i, f in enumerate(files):
                        all_transcripts.extend(SubtitleProcessor.parse_file_path(f))
                        prog.progress((i+1)/len(files))
                    
                    service.index_data(all_transcripts)
                    status.update(label="‚úÖ –î–µ–º–æ –¥–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã!", state="complete", expanded=False)
                    st.rerun()

        else:
            uploaded_files = st.file_uploader("–ü–µ—Ä–µ—Ç–∞—â–∏—Ç–µ .srt —Ñ–∞–π–ª—ã —Å—é–¥–∞", type=["srt"], accept_multiple_files=True)
            if uploaded_files:
                if st.button(f"üöÄ –ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å {len(uploaded_files)} —Ñ–∞–π–ª–æ–≤", type="primary"):
                    with st.status("–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤...", expanded=True) as status:
                        all_transcripts = []
                        service.reset_collection()
                        
                        prog = st.progress(0)
                        for i, file in enumerate(uploaded_files):
                            transcripts = SubtitleProcessor.parse_uploaded_file(file)
                            all_transcripts.extend(transcripts)
                            prog.progress((i+1)/len(uploaded_files))
                        
                        if all_transcripts:
                            st.write(f"–ó–∞–≥—Ä—É–∑–∫–∞ {len(all_transcripts)} —Å—Ç—Ä–æ–∫ –≤ –ë–î...")
                            service.index_data(all_transcripts)
                            status.update(label="‚úÖ –í–∞—à–∏ —Ñ–∞–π–ª—ã –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω—ã!", state="complete", expanded=False)
                            st.rerun()
                        else:
                            st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª—ã.")

        st.markdown("---")
        st.metric("–í –±–∞–∑–µ —Å—Ç—Ä–æ–∫:", service.get_stats())

    # Main Area
    st.title("üîé –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ —Å—É–±—Ç–∏—Ç—Ä–∞–º")
    
    if service.get_stats() == 0:
        st.warning("üëà –ë–∞–∑–∞ –ø—É—Å—Ç–∞. –í—ã–±–µ—Ä–∏—Ç–µ –∏—Å—Ç–æ—á–Ω–∏–∫ —Å–ª–µ–≤–∞ –∏ –Ω–∞–∂–º–∏—Ç–µ –∫–Ω–æ–ø–∫—É –ò–Ω–¥–µ–∫—Å–∞—Ü–∏–∏.")
    else:
        q = st.text_input("–ó–∞–ø—Ä–æ—Å (–º–æ–∂–Ω–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º)", placeholder="–û —á–µ–º –≥–æ–≤–æ—Ä—è—Ç –≥–µ—Ä–æ–∏?", label_visibility="collapsed")
        
        limit = st.selectbox(
            "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤", 
            options=range(1, 100),
            index=4
        )
        
        if q:
            with st.spinner("–ü–æ–∏—Å–∫..."):
                res = service.search(q, limit)
            if res:
                for r in res:
                    render_result(r)
            else:
                st.info("–ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")

if __name__ == "__main__":
    main()
